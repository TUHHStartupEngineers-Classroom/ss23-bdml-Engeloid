---
title: "Supervised ML - Regression"
date: "2023-06-12"
output: 
    html_document:
        toc: TRUE
        theme: flatly
        highlight: tango
        code_folding: hide
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```

# Challenge Summary

In this session we did not use the recipes packages to prepare our data. This is going to be your challenge. For further information take a look at the last session or just use google. Prepare the data for the models with the steps provided below. Remember, you don’t need to set the flags by yourself (see all_nominal()).

# Libraries

Load the following libraries. 


```{r}
# install.packages("plotly")

library(tidyverse)
library(tidyquant)
library(broom)
library(umap)
library(ggrepel) # Addon for ggplot, so that the labels do not overlap
library(vip)
library(workflows)
library(glmnet)

# Modeling
library(parsnip)

# Preprocessing & Sampling
library(recipes)
library(rsample)

# Modeling Error Metrics
library(yardstick)
library(ranger)

# Plotting Decision Trees
library(rpart.plot)
```

# Helper functions


```{r}
plot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {
  
  color_expr <- enquo(color)
  if (rlang::quo_is_null(color_expr)) {
    
    g <- data %>%
      ggpairs(lower = "blank") 
    
  } else {
    
    color_name <- quo_name(color_expr)
    g <- data %>%
      ggpairs(mapping = aes_string(color = color_name), 
              lower = "blank", legend = 1,
              diag = list(continuous = wrap("densityDiag", 
                                            alpha = density_alpha))) +
      theme(legend.position = "bottom")
  }
  
  return(g)
}
```


# Data

```{r}

# Data set
bike_orderlines_tbl <- readRDS("bike_orderlines.rds")
glimpse(bike_orderlines_tbl)

```
```{r}
model_sales_tbl <- bike_orderlines_tbl %>%
    select(total_price, model, category_2, frame_material) %>%
    group_by(model, category_2, frame_material) %>%
    summarise(total_sales = sum(total_price)) %>%
    ungroup() %>%
    arrange(desc(total_sales))

model_sales_tbl
```
```{r}

model_sales_tbl %>%
    mutate(category_2 = as_factor(category_2) %>% 
               fct_reorder(total_sales, .fun = max) %>% 
               fct_rev()) %>%
    ggplot(aes(frame_material, total_sales)) +
    geom_violin() +
    geom_jitter(width = 0.1, alpha = 0.5, color = "#2c3e50") +
    #coord_flip() +
    facet_wrap(~ category_2) +
    scale_y_continuous(labels = scales::dollar_format(scale = 1e-6, suffix = "M", accuracy = 0.1)) +
    tidyquant::theme_tq() +
    labs(
        title = "Total Sales for Each Model",
        x = "Frame Material", y = "Revenue"
    )

```

## Data Preparation & Feature Engineering
```{r}
bike_features_tbl <- readRDS("bike_features_tbl.rds")
glimpse(bike_features_tbl)

bike_features_tbl <- bike_features_tbl %>% 
    select(model:url, `Rear Derailleur`, `Shift Lever`) %>% 
    mutate(
      `shimano dura-ace`        = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano dura-ace ") %>% as.numeric(),
      `shimano ultegra`         = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano ultegra ") %>% as.numeric(),
      `shimano 105`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano 105 ") %>% as.numeric(),
      `shimano tiagra`          = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano tiagra ") %>% as.numeric(),
      `Shimano sora`            = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano sora") %>% as.numeric(),
      `shimano deore`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano deore(?! xt)") %>% as.numeric(),
      `shimano slx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano slx") %>% as.numeric(),
      `shimano grx`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano grx") %>% as.numeric(),
      `Shimano xt`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano deore xt |shimano xt ") %>% as.numeric(),
      `Shimano xtr`             = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano xtr") %>% as.numeric(),
      `Shimano saint`           = `Rear Derailleur` %>% str_to_lower() %>% str_detect("shimano saint") %>% as.numeric(),
      `SRAM red`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram red") %>% as.numeric(),
      `SRAM force`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram force") %>% as.numeric(),
      `SRAM rival`              = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram rival") %>% as.numeric(),
      `SRAM apex`               = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram apex") %>% as.numeric(),
      `SRAM xx1`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram xx1") %>% as.numeric(),
      `SRAM x01`                = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram x01|sram xo1") %>% as.numeric(),
      `SRAM gx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram gx") %>% as.numeric(),
      `SRAM nx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram nx") %>% as.numeric(),
      `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram sx") %>% as.numeric(),
      `SRAM sx`                 = `Rear Derailleur` %>% str_to_lower() %>% str_detect("sram sx") %>% as.numeric(),
      `Campagnolo potenza`      = `Rear Derailleur` %>% str_to_lower() %>% str_detect("campagnolo potenza") %>% as.numeric(),
      `Campagnolo super record` = `Rear Derailleur` %>% str_to_lower() %>% str_detect("campagnolo super record") %>% as.numeric(),
      `shimano nexus`           = `Shift Lever`     %>% str_to_lower() %>% str_detect("shimano nexus") %>% as.numeric(),
      `shimano alfine`          = `Shift Lever`     %>% str_to_lower() %>% str_detect("shimano alfine") %>% as.numeric()
    ) %>% 
  # Remove original columns  
  select(-c(`Rear Derailleur`, `Shift Lever`)) %>% 
  # Set all NAs to 0
  mutate_if(is.numeric, ~replace(., is.na(.), 0))
```
Reordering and tidying 

```{r}
# 2.0 TRAINING & TEST SETS ----
bike_features_tbl <- bike_features_tbl %>% 
  
  mutate(id = row_number()) %>% 
  
  select(id, everything(), -url)
```
## Splitting the Data

```{r}
bike_features_tbl %>% distinct(category_2)

# run both following commands at the same time
set.seed(seed = 1337)
split_obj <- rsample::initial_split(bike_features_tbl, prop   = 0.80, 
                                                       strata = "category_2")

# Check if testing contains all category_2 values
split_obj %>% training() %>% distinct(category_2)
split_obj %>% testing() %>% distinct(category_2)

# Assign training and test data
train_tbl <- training(split_obj)
test_tbl  <- testing(split_obj) 

# We have to remove spaces and dashes from the column names
train_tbl <- train_tbl %>% set_names(str_replace_all(names(train_tbl), " |-", "_"))
test_tbl  <- test_tbl  %>% set_names(str_replace_all(names(test_tbl),  " |-", "_"))

```
## Recipe

```{r}
bike_rec <- 
  recipe(price ~ ., data = train_tbl) %>%
  step_rm(id, model, model_year, gender, category_1, category_3) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>%
  step_zv()

```


# Linear Regression

## Model 1

### Model

```{r}
# 3.0 LINEAR METHODS ----
# 3.1 LINEAR REGRESSION - NO ENGINEERED FEATURES ----

# 3.1.1 Model ----
model_01_model <- 
  linear_reg(mode = "regression") %>%
  set_engine("lm")

model_01_workflow <- 
  workflow() %>%
  add_model(model_01_model) %>%
  add_recipe(bike_rec)

model_01_linear_lm_simple <- 
  model_01_workflow %>%
  fit(data = train_tbl)

model_01_linear_lm_simple %>%
    predict(new_data = test_tbl) %>%
    bind_cols(test_tbl %>% select(price)) %>%
    yardstick::metrics(truth = price, estimate = .pred_res)
```
### Model Explanation
```{r}
# tidy() function is applicable for objects with class "lm"
model_01_linear_lm_simple %>%
  pull_workflow_fit() %>%
  broom::tidy() %>%
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 01: Simple lm Model") 
```

```{r}
# 3.1.3 Function to Calculate Metrics ----

# Code we used earlier
model_01_linear_lm_simple %>%
    predict(new_data = test_tbl) %>%

    bind_cols(test_tbl %>% select(price)) %>%
    yardstick::metrics(truth = price, estimate = .pred_res)

# Generalized into a function
calc_metrics <- function(model, new_data = test_tbl) {

    model %>%
        predict(new_data = new_data) %>%

        bind_cols(new_data %>% select(price)) %>%
        yardstick::metrics(truth = price, estimate = .pred_res)

}

model_01_linear_lm_simple %>% calc_metrics(test_tbl)
```
## Model 2

### Model

```{r}
# 3.2 LINEAR REGRESSION - WITH ENGINEERED FEATURES ----

# 3.2.1 Model ----
model_02_model <- linear_reg("regression") %>%
    set_engine("lm")

model_02_workflow <- 
  workflow() %>%
  add_model(model_02_model) %>%
  add_recipe(bike_rec)

model_02_linear_lm_complex <- 
  model_02_workflow %>%
  fit(data = train_tbl)

model_02_linear_lm_complex %>% calc_metrics(test_tbl)
```

### Model Explanation
```{r}
# 3.2.2 Feature importance ----
model_02_linear_lm_complex %>%
  pull_workflow_fit() %>%
  broom::tidy() %>%
  arrange(p.value) %>%
  mutate(term = as_factor(term) %>% fct_rev()) %>%
  
  ggplot(aes(x = estimate, y = term)) +
  geom_point(color = "#2dc6d6", size = 3) +
  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = " €", prefix = "")),
                            size = 4, fill = "#272A36", color = "white") +
  scale_x_continuous(labels = scales::dollar_format(suffix = " €", prefix = "")) +
  labs(title = "Linear Regression: Feature Importance",
       subtitle = "Model 02: Complex lm Model")
```

# GLM Regularized Regression

## Model 3: GLMNET (Elastic Net)

### Model
```{r}
# 3.3 PENALIZED REGRESSION ----

# 3.3.1 Model ----
model_03_model <- linear_reg(mode    = "regression", 
             penalty = 10, 
             mixture = 0.2) %>%
  set_engine("glmnet")

model_03_workflow <- 
  workflow() %>%
  add_model(model_03_model) %>%
  add_recipe(bike_rec)

model_03_linear_glmnet <- 
  model_03_workflow %>%
  fit(data = train_tbl)

model_03_linear_glmnet %>% 
  predict(test_tbl) %>%
  bind_cols(test_tbl %>% select(price)) %>%
  yardstick::metrics(truth = price, estimate = .pred)
```
### Model Explanation

```{r}
# 3.3.2 Feature Importance ----
model_03_linear_glmnet %>%
    pull_workflow_fit() %>%
    broom::tidy() %>%
    
    # No p value here
    arrange(desc(abs(estimate))) %>%
    mutate(term = as_factor(term) %>% fct_rev()) %>%

    ggplot(aes(x = estimate, y = term)) +
    geom_point() +
    ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1)),
                              size = 3) +
    scale_x_continuous(labels = scales::dollar_format()) +
    labs(title = "Linear Regression: Feature Importance",
         subtitle = "Model 03: GLMNET Model")
```

# Tree-Based Methods

## Model 4: Decision Trees

### Model

```{r}
# 4.0 TREE-BASED METHODS ----
# 4.1 DECISION TREES ----
# 4.1.1 Model ----

model_04_model <- 
  decision_tree(mode = "regression",
              
              # Set the values accordingly to get started
              cost_complexity = 0.001,
              tree_depth      = 5,
              min_n           = 7) %>%
  set_engine("rpart")

model_04_workflow <- 
  workflow() %>%
  add_model(model_04_model) %>%
  add_recipe(bike_rec)

model_04_tree_decision_tree <- 
  model_04_workflow %>%
  fit(data = train_tbl)

model_04_tree_decision_tree %>% 
  predict(test_tbl) %>%
  bind_cols(test_tbl %>% select(price)) %>%
  yardstick::metrics(truth = price, estimate = .pred)
```
### Model Explanation

```{r}
pull_workflow_fit(model_04_tree_decision_tree)$fit %>%
    rpart.plot(roundint = FALSE)

# Optimze plot
pull_workflow_fit(model_04_tree_decision_tree)$fit %>%
    rpart.plot(
        roundint = FALSE,
        type = 1,
        extra = 101, # see help page
        fallen.leaves = FALSE, # changes the angles from 90 to 45-degree
        cex = 0.8, # font size
        main = "Model 04: Decision Tree", # Adds title
        box.palette = "Blues"
        )

show.prp.palettes()
```

## Ranger - Model 5

### Model

```{r}
# 4.2 RANDOM FOREST ----

# 4.2.1 Model: ranger ----
library(ranger)

model_05_model <- rand_forest(
      mode = "regression", mtry = 8, trees = 5000, min_n = 10
    ) %>%
    set_engine("ranger", replace = TRUE, splitrule = "extratrees", importance = "impurity")

model_05_workflow <- 
  workflow() %>%
  add_model(model_05_model) %>%
  add_recipe(bike_rec)

model_05_rand_forest_ranger <- 
  model_05_workflow %>%
  fit(data = train_tbl)

model_05_rand_forest_ranger %>% 
  predict(test_tbl) %>%
  bind_cols(test_tbl %>% select(price)) %>%
  yardstick::metrics(truth = price, estimate = .pred)
```

### Model Explanation

```{r}
# 4.2.2 ranger: Feature Importance ----

pull_workflow_fit(model_05_rand_forest_ranger)$fit %>%
    ranger::importance() %>%
    enframe() %>%
    arrange(desc(value)) %>%
    mutate(name = as_factor(name) %>% fct_rev()) %>%

    ggplot(aes(value, name)) +
    geom_point() +
    labs(title = "ranger: Variable Importance",
         subtitle = "Model 05: Ranger Random Forest Model")
```

## randomForest - Model 06

### Model

```{r}
# 4.2.3 Model randomForest ----
model_06_model <- 
    rand_forest("regression") %>%
    set_engine("randomForest")

model_06_workflow <- 
  workflow() %>%
  add_model(model_06_model) %>%
  add_recipe(bike_rec)

model_06_rand_forest_randomForest <- 
  model_06_workflow %>%
  fit(data = train_tbl)

model_06_rand_forest_randomForest %>% 
  predict(test_tbl) %>%
  bind_cols(test_tbl %>% select(price)) %>%
  yardstick::metrics(truth = price, estimate = .pred)
```
### Model Explanation
```{r}
pull_workflow_fit(model_06_rand_forest_randomForest)$fit %>%
    randomForest::importance() %>%
    # Convert matrix to tibble
    as_tibble(rownames = "name") %>%
    arrange(desc(IncNodePurity)) %>%
    mutate(name = as_factor(name) %>% fct_rev()) %>%

    ggplot(aes(IncNodePurity, name)) +
    geom_point() +
    labs(
        title = "randomForest: Variable Importance",
        subtitle = "Model 06: randomForest Model"
    )
```

## Model - XGBoost

```{r}
# 4.3 XGBOOST ----
# 4.3.1 Model ----

model_07_model <- 
    boost_tree(
      mode = "regression",
      mtry = 30,
      learn_rate = 0.25,
      tree_depth = 7
    ) %>%
    set_engine("xgboost")

model_07_workflow <- 
  workflow() %>%
  add_model(model_07_model) %>%
  add_recipe(bike_rec)

model_07_boost_tree_xgboost <- 
  model_07_workflow %>%
  fit(data = train_tbl)

model_07_boost_tree_xgboost %>% 
  predict(test_tbl) %>%
  bind_cols(test_tbl %>% select(price)) %>%
  yardstick::metrics(truth = price, estimate = .pred)
```
### Model Explanation

```{r}
# 4.3.2 Feature Importance ----

pull_workflow_fit(model_07_boost_tree_xgboost)$fit %>%
    xgboost::xgb.importance(model = .) %>%
    as_tibble() %>%
    arrange(desc(Gain)) %>%
    mutate(Feature = as_factor(Feature) %>% fct_rev()) %>%

    ggplot(aes(Gain, Feature)) +
    geom_point() +
    labs(
        title = "XGBoost: Variable Importance",
        subtitle = "Model 07: XGBoost Model"
    )
```

# Prediction & Evaluation

```{r}
# 5.0 TESTING THE ALGORITHMS OUT ----
g1 <- bike_features_tbl %>% 
    mutate(category_2 = as.factor(category_2) %>% 
           fct_reorder(price)) %>% 
    
    ggplot(aes(category_2, price)) +
    geom_violin() +
    geom_jitter(width = 0.1, alpha = 0.5, color = "#2dc6d6") +
    coord_flip() +
    facet_wrap(~ frame_material) +
    scale_y_continuous(labels = scales::dollar_format()) +
    labs(
        title = "Unit Price for Each Model",
        y = "", x = "Category 2"
    )
g1
```

```{r}
# 5.1 NEW MODEL ----

new_cross_country <- tibble(
        id = 0,
        model = "Exceed AL SL new",
        model_year = 2022,
        frame_material = "aluminium",
        weight = 10,
        category_1 = "Mountain",
        category_2 = "Cross-Country",
        category_3 = "Exceed",
        gender = "unisex",
        shimano_dura_ace = 0,
        shimano_ultegra = 0,
        shimano_105 = 0,
        shimano_tiagra = 0,
        Shimano_sora = 0,
        shimano_deore = 0,
        shimano_slx = 0,
        shimano_grx = 0,
        Shimano_xt = 1,
        Shimano_xtr = 0,
        Shimano_saint = 0,
        SRAM_red = 0,
        SRAM_force = 0,
        SRAM_rival = 0,
        SRAM_apex = 0,
        SRAM_xx1 = 0,
        SRAM_x01 = 0,
        SRAM_gx = 0,
        SRAM_nx = 0,
        SRAM_sx = 0,
        Campagnolo_potenza = 0,
        Campagnolo_super_record = 0,
        shimano_nexus = 0,
        shimano_alfine = 0
) 

new_cross_country
```

```{r}
models_tbl <- tibble(
    model_id = str_c("Model 0", 1:7),
    model = list(
        model_01_linear_lm_simple,
        model_02_linear_lm_complex,
        model_03_linear_glmnet,
        model_04_tree_decision_tree,
        model_05_rand_forest_ranger,
        model_06_rand_forest_randomForest,
        model_07_boost_tree_xgboost
    )
)

models_tbl

# Add Predictions

predictions_new_cross_country_tbl <- models_tbl %>%
    mutate(predictions = map(model, predict, new_data = new_cross_country)) %>%
    unnest(predictions) %>%
    mutate(category_2 = "Cross-Country") %>%
    left_join(new_cross_country, by = "category_2")
    
predictions_new_cross_country_tbl

```



Congratulations! You are done with the 2nd challenge!