---
title: "Deep Learning"
date: "2023-06-13"
output: 
    html_document:
        toc: TRUE
        theme: flatly
        highlight: tango
        code_folding: hide
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE
    )
```

# Challenge Summary

A telecommunications company [Telco] is concerned about the number of customers leaving their landline business for cable competitors. They need to understand who is leaving. Imagine that you’re an analyst at this company and you have to find out who is leaving and why.

Customer churn refers to the situation when a customer ends their relationship with a company, and it’s a costly problem. Customer churn is a problem that all companies need to monitor, especially those that depend on subscription-based revenue streams. Loss of customers impacts sales. We are using the keras package to produce an Artificial Neural Network (ANN) model on the IBM Watson Telco Customer Churn Data Set! As for most business problems, it’s equally important to explain what features drive the model, which is why we’ll use the lime package for explainability. Moreover, we are going to cross-check the LIME results with a Correlation Analysis.

# Libraries

Load the following libraries. 


```{r}
# install.packages("plotly")

library(tidyverse)
library(tidyquant)
library(broom)
library(umap)
library(ggrepel) # Addon for ggplot, so that the labels do not overlap
library(corrr)
library(lime)

# Modeling
library(parsnip)

# Preprocessing & Sampling
library(recipes)
library(rsample)

# Modeling Error Metrics
library(yardstick)

# Plotting Decision Trees
library(rpart.plot)

#Deep Learning
library(tensorflow)
install_tensorflow(version = "2.12.0", method = "conda", envname = "r-reticulate")
library(keras)
use_condaenv("r-reticulate")
#install_tensorflow()
#install_keras()
```


# Data

```{r}

# Data set
churn_data_raw <- as.tibble(read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv"))
glimpse(churn_data_raw)

```
# Preprocessing
First, we “prune” the data, which is nothing more than removing unnecessary columns and rows. The data has a few columns and rows we’d like to remove:

 - The “customerID” column is a unique identifier for each observation that isn’t needed for modeling.
 - The data has 11 NA values all in the “TotalCharges” column. Because it’s such a small percentage of the total population (99.8% complete cases), we can drop these observations (the tidyr package provides a function for that. Type tidyr:: to get a list of the function.)
 - have the target in the first column
```{r}
churn_data_tbl <- churn_data_raw %>%
                  select(-customerID) %>%
                  tidyr::drop_na(TotalCharges) %>% 
                  relocate(Churn)

churn_data_tbl
```
## Split data
```{r}
# Split test/training sets
set.seed(100)
train_test_split <- rsample::initial_split(churn_data_tbl, prop = 0.85)
train_test_split

## <Analysis/Assess/Total>
## <5626/1406/7032>

# Retrieve train and test sets
train_tbl <- training(train_test_split)
test_tbl  <- testing(train_test_split)
```

## Histogram
```{r}
#hist(churn_data_tbl)
churn_data_tbl %>% ggplot(aes(x = tenure)) +
                      geom_histogram(binwidth = 0.5, fill =  "#2DC6D6") +
                      labs(
                        title = "Tenure Counts Without Binning",
                        x     = "tenure (month)"
                        )
```


```{r}
churn_data_tbl %>% ggplot(aes(x = tenure)) + 
  geom_histogram(bins = 6, color = "white", fill =  "#2DC6D6") +
  labs(
    title = "Tenure Counts With Six Bins",
    x     = "tenure (month)"
  )
```


```{r}
# Determine if log transformation improves correlation 
# between TotalCharges and Churn

train_tbl %>%
    select(Churn, TotalCharges) %>%
    mutate(
        Churn = Churn %>% as.factor() %>% as.numeric(),
        LogTotalCharges = log(TotalCharges)
        ) %>%
    corrr::correlate() %>%
    corrr::focus(Churn) %>%
    corrr::fashion()
```


# One-Hot Encoding
```{r}
churn_data_tbl %>% 
        pivot_longer(cols      = c(Contract, InternetService, MultipleLines, PaymentMethod), 
                     names_to  = "feature", 
                     values_to = "category") %>% 
        ggplot(aes(category)) +
          geom_bar(fill = "#2DC6D6") +
          facet_wrap(~ feature, scales = "free") +
          labs(
            title = "Features with multiple categories: Need to be one-hot encoded"
          ) +
          theme(axis.text.x = element_text(angle = 25, 
                                           hjust = 1))
```


# Preprocessing With Recipes
```{r}
rec_obj <- recipe(Churn ~ ., data = train_tbl) %>%
    step_rm(Churn) %>% 
    step_discretize(tenure, options = list(cuts = 6)) %>%
    step_log(TotalCharges) %>%
    step_dummy(all_nominal(), -all_outcomes(), one_hot = T) %>%
    step_center(all_predictors(), -all_outcomes()) %>%
    step_scale(all_predictors(), -all_outcomes()) %>%
    prep(data = train_tbl)
```



```{r}
# Predictors
x_train_tbl <- bake(rec_obj, new_data = train_tbl)
x_test_tbl  <- bake(rec_obj, new_data = test_tbl)
#as.matrix(x_train_tbl)
x_train_tbl
```



```{r}
# Response variables for training and testing sets
#y_train_vec <- ifelse( ... )
#y_test_vec  <- ifelse( ... )
```


## Keras
# Unfortunately I did not get to install tensorflow properly, so it doesn't work
## After so many trys...:
 - https://community.rstudio.com/t/error-installation-of-tensorflow-not-found-in-rstudio/67200
 - https://github.com/rstudio/tensorflow
 - https://community.rstudio.com/t/problem-in-configuration-rstudio-with-python/44235/4
 - https://community.rstudio.com/t/keras-error-python-module-tensorflow-keras-was-not-found/93023/2
 - https://stackoverflow.com/questions/44611325/r-keras-package-error-python-module-tensorflow-contrib-keras-python-keras-was-n
```{r}
# Building our Artificial Neural Network
model_keras <- keras_model_sequential()

model_keras %>% 
    # First hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu", 
        input_shape        = ncol(x_train_tbl)) %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Second hidden layer
    layer_dense(
        units              = 16, 
        kernel_initializer = "uniform", 
        activation         = "relu") %>% 
    # Dropout to prevent overfitting
    layer_dropout(rate = 0.1) %>%
    # Output layer
    layer_dense(
        units              = 1, 
        kernel_initializer = "uniform", 
        activation         = "sigmoid") %>% 
    # Compile ANN
    compile(
        optimizer = 'adam',
        loss      = 'binary_crossentropy',
        metrics   = c('accuracy')
    )
model_keras
```



```{r}
# fit_keras <- fit(
#     object = model_keras,
#     x = as.matrix(x_train_tbl),
#     #y = ,
#     batch_size = 50,
#     epochs = 35,
#     validation_split = 0.3
#     )
```



```{r}
 # fit_keras
```



```{r}
# plot(fit_keras) +
#   labs(title = "Deep Learning Training Results") +
#   theme(legend.position  = "bottom",
#         strip.placement  = "inside",
#         strip.background = element_rect(fill = "#grey"))
```



```{r}
# # Predicted Class
# yhat_keras_class_vec <- predict_classes(object = model_keras, x = as.matrix(x_test_tbl)) %>%
#     as.vector()
# 
# # Predicted Class Probability
# yhat_keras_prob_vec  <- predict_proba(object = model_keras, x = as.matrix(x_test_tbl)) %>%
#     as.vector()
```



```{r}
# estimates_keras_tbl <- tibble(
#     truth      = as.factor(y_test_vec) %>% fct_recode(yes = "1", no = "0"),
#     estimate   = as.factor(yhat_keras_class_vec) %>% fct_recode(yes = "1", no = "0"),
#     class_prob = yhat_keras_prob_vec
# )
# 
# estimates_keras_tbl
```



```{r}
# Confusion Table
#... %>% ...
```



```{r}
# Accuracy
#... %>% ...
```



```{r}
# AUC
#... %>% ...
```



```{r}
# Precision
# tibble(
#     precision = ...,
#     recall    = ...
# )
```



```{r}
# F1-Statistic
#estimates_keras_tbl %>% f_meas(truth, estimate, beta = 1)
```



```{r}
#class(model_keras)
```



```{r}
# model_type.keras.engine.sequential.Sequential  <- function(x, ...) {
#     return("classification")
# }
```



```{r}
# Setup lime::predict_model() function for keras
# predict_model.keras.engine.sequential.Sequential <- function(x, newdata, type, ...) {
#     pred <- predict_proba(object = x, x = as.matrix(newdata))
#     return(data.frame(Yes = pred, No = 1 - pred))
# }
```



```{r}
# # Test our predict_model() function
# predict_model(x = model_keras, newdata = x_test_tbl, type = 'raw') %>%
#     tibble::as_tibble()
```



```{r}
# # Run lime() on training set
# explainer <- lime::lime(
#     ...            = ..., 
#     ...            = ... , 
#     bin_continuous = FALSE)
```



```{r}
# explanation <- lime::explain(
#     x_test_tbl[1:10,], 
#     ...    = ..., 
#     ...    = ..., 
#     ...    = ...,
#     ...    = ...)
```



```{r}
# # Feature correlations to Churn
# corrr_analysis <- x_train_tbl %>%
#     mutate(Churn = y_train_vec) %>%
#     correlate() %>%
#     focus(Churn) %>%
#     rename(feature = rowname) %>%
#     arrange(abs(Churn)) %>%
#     mutate(feature = as_factor(feature)) 
# corrr_analysis
```



```{r}
# # Correlation visualization
# corrr_analysis %>%
#   ggplot(aes(x = ..., y = fct_reorder(..., desc(...)))) +
#   geom_point() +
#   
#   # Positive Correlations - Contribute to churn
#   geom_segment(aes(xend = ..., yend = ...), 
#                color = "red", 
#                data = corrr_analysis %>% filter(... > ...)) +
#   geom_point(color = "red", 
#              data = corrr_analysis %>% filter(... > ...)) +
#   
#   # Negative Correlations - Prevent churn
#   geom_segment(aes(xend = 0, yend = feature), 
#                color = "#2DC6D6", 
#                data = ... +
#   geom_point(color = "#2DC6D6", 
#              data = ... +
#   
#   # Vertical lines
#   geom_vline(xintercept = 0, color = "#f1fa8c", size = 1, linetype = 2) +
#   geom_vline( ... ) +
#   geom_vline( ... ) +
#   
#   # Aesthetics
#   labs( ... )
```


Congratulations! You are done with the 7th challenge!